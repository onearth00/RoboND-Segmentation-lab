{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segmentation Lab\n",
    "In this lab, you will build a deep learning network that locates a particular human target within an image.  The premise is that a quadcopter (simulated) is searching for a target, and then will follow the target once found.  It's not enough to simply say the target is present in the image in this case, but rather to know *where* in the image the target is, so that the copter can adjust its direction in order to follow.\n",
    "\n",
    "Consequently, an image classification network is not enough to solve the problem. Instead, a semantic segmentation network is needed so that the target can be specifically located within the image.\n",
    "\n",
    "You can click on any of the following to quickly jump to that part of this notebook:\n",
    "1. [Data Collection](#data)\n",
    "2. [FCN Layers](#fcn)\n",
    "3. [Build the Model](#build)\n",
    "4. [Training](#training)\n",
    "5. [Prediction](#prediction)\n",
    "6. [Evaluation](#evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection<a id='data'></a>\n",
    "We have provided you with the dataset for this lab. If you haven't already downloaded the training and validation datasets, you can check out the README for this lab's repo for instructions as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "from scipy import misc\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.contrib.keras.python import keras\n",
    "from tensorflow.contrib.keras.python.keras import layers, models\n",
    "\n",
    "from tensorflow import image\n",
    "\n",
    "from utils import scoring_utils\n",
    "from utils.separable_conv2d import SeparableConv2DKeras, BilinearUpSampling2D\n",
    "from utils import data_iterator\n",
    "from utils import plotting_tools \n",
    "from utils import model_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FCN Layers<a id='fcn'></a>\n",
    "In the Classroom, we discussed the different layers that constitute a fully convolutional network. The following code will introduce you to the functions that you will be using to build out your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separable Convolutions\n",
    "The Encoder for your FCN will essentially require separable convolution layers. Below we have implemented two functions - one which you can call upon to build out separable convolutions or regular convolutions. Each with batch normalization and with the ReLU activation function applied to the layers. \n",
    "\n",
    "While we recommend the use of separable convolutions thanks to their advantages we covered in the Classroom, some of the helper code we will present for your model will require the use for regular convolutions. But we encourage you to try and experiment with each as well!\n",
    "\n",
    "The following will help you create the encoder block and the final model for your architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def separable_conv2d_batchnorm(input_layer, filters, strides=1):\n",
    "    output_layer = SeparableConv2DKeras(filters=filters,kernel_size=3, strides=strides,\n",
    "                             padding='same', activation='relu')(input_layer)\n",
    "    \n",
    "    output_layer = layers.BatchNormalization()(output_layer) \n",
    "    return output_layer\n",
    "\n",
    "def conv2d_batchnorm(input_layer, filters, kernel_size=3, strides=1):\n",
    "    output_layer = layers.Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, \n",
    "                      padding='same', activation='relu')(input_layer)\n",
    "    \n",
    "    output_layer = layers.BatchNormalization()(output_layer) \n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bilinear Upsampling\n",
    "The following helper function will help implement the bilinear upsampling layer. Currently, upsampling by a factor of 2 is recommended but you can try out different factors as well. You will use this to create the decoder block later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bilinear_upsample(input_layer):\n",
    "    output_layer = BilinearUpSampling2D((2,2))(input_layer)\n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model<a id='build'></a>\n",
    "In the following cells, we will cover how to build the model for the task at hand. \n",
    "\n",
    "- We will first create an Encoder Block, where you will create a separable convolution layer using an input layer and the size(depth) of the filters as your inputs.\n",
    "- Next, you will create the Decoder Block, where you will create an upsampling layer using bilinear upsampling, followed by a layer concatentaion, and some separable convolution layers.\n",
    "- Finally, you will combine the above two and create the model. In this step you will be able to experiment with different number of layers and filter sizes for each to build your model.\n",
    "\n",
    "Let's cover them individually below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Block\n",
    "Below you will create a separable convolution layer using the separable_conv2d_batchnorm() function. The `filters` parameter defines the size or depth of the output layer. For example, 32 or 64. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoder_block(input_layer, filters, strides):\n",
    "    \n",
    "    # TODO Create a separable convolution layer using the separable_conv2d_batchnorm() function.\n",
    "    output_layer = separable_conv2d_batchnorm(input_layer,filters,strides)\n",
    "    \n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Block\n",
    "The decoder block, as covered in the Classroom, comprises of three steps -\n",
    "\n",
    "- A bilinear upsampling layer using the bilinear_upsample() function. The current recommended factor for upsampling is set to 2.\n",
    "- A layer concatenation step. This step is similar to skip connections. You will concatenate the upsampled small_ip_layer and the large_ip_layer.\n",
    "- Some (one or two) additional separable convolution layers to extract some more spatial information from prior layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoder_block(small_ip_layer, large_ip_layer, filters):\n",
    "    \n",
    "    # TODO Upsample the small input layer using the bilinear_upsample() function.\n",
    "    out = bilinear_upsample(small_ip_layer)\n",
    "    \n",
    "    # TODO Concatenate the upsampled and large input layers using layers.concatenate\n",
    "    out = layers.concatenate([out, large_ip_layer])\n",
    "    \n",
    "    # TODO Add some number of separable convolution layers\n",
    "    output_layer = separable_conv2d_batchnorm(out,filters)\n",
    "    \n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Now that you have the encoder and decoder blocks ready, you can go ahead and build your model architecture! \n",
    "\n",
    "There are three steps to the following:\n",
    "- Add encoder blocks to build out initial set of layers. This is similar to how you added regular convolutional layers in your CNN lab.\n",
    "- Add 1x1 Convolution layer using conv2d_batchnorm() function. Remember that 1x1 Convolutions require a kernel and stride of 1.\n",
    "- Add decoder blocks for upsampling and skip connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fcn_model(inputs, num_classes):\n",
    "    \n",
    "    # TODO Add Encoder Blocks. \n",
    "    l1 = encoder_block(inputs, 32, 2)\n",
    "    l2 = encoder_block(l1,64,2)\n",
    "    # Remember that with each encoder layer, the depth of your model (the number of filters) increases.\n",
    "\n",
    "    # TODO Add 1x1 Convolution layer using conv2d_batchnorm().\n",
    "    l3 = conv2d_batchnorm(l2, 128, kernel_size=1, strides=1)\n",
    "    \n",
    "    # TODO: Add the same number of Decoder Blocks as the number of Encoder Blocks\n",
    "    l4 = decoder_block(l3,l1,64)\n",
    "    x = decoder_block(l4,inputs,32)\n",
    "    # The function returns the output layer of your model. \"x\" is the final layer obtained from the last decoder_block()\n",
    "    return layers.Conv2D(num_classes, 3, activation='softmax', padding='same')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training<a id='training'></a>\n",
    "The following cells will utilize the model you created and define an ouput layer based on the input and the number of classes.Following that you will define the hyperparameters to compile and train your model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "image_hw = 128\n",
    "image_shape = (image_hw, image_hw, 3)\n",
    "inputs = layers.Input(image_shape)\n",
    "num_classes = 3\n",
    "\n",
    "# Call fcn_model()\n",
    "output_layer = fcn_model(inputs, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Define and tune your hyperparameters.\n",
    "- **batch_size**: number of training samples/images that get propagated through the network in a single pass.\n",
    "- **num_epochs**: number of times the entire training dataset gets propagated through the network.\n",
    "- **steps_per_epoch**: number of batches of training images that go through the network in 1 epoch. We have provided you with a default value. One recommended value to try would be based on the total number of images in training dataset divided by the batch_size.\n",
    "- **validation_steps**: number of batches of validation images that go through the network in 1 epoch. This is similar to steps_per_epoch, except validation_steps is for the validation dataset. We have provided you with a default value for this as well.\n",
    "- **workers**: maximum number of processes to spin up. This can affect your training speed and is dependent on your hardware. We have provided a recommended value to work with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "batch_size = 128\n",
    "num_epochs = 1\n",
    "\n",
    "steps_per_epoch = 500\n",
    "validation_steps = 50\n",
    "workers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.0704"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa9bf780630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 487s - loss: 0.0703 - val_loss: 0.1993\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.contrib.keras.python.keras.callbacks.History at 0x7fa9c13e5a58>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "# Define the Keras model and compile it for training\n",
    "model = models.Model(inputs=inputs, outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate), loss='categorical_crossentropy')\n",
    "\n",
    "# Data iterators for loading the training and validation data\n",
    "train_iter = data_iterator.BatchIteratorSimple(batch_size=batch_size,\n",
    "                                               data_folder=os.path.join('..', 'data', 'train_combined'),\n",
    "                                               image_shape=image_shape,\n",
    "                                               shift_aug=True)\n",
    "\n",
    "val_iter = data_iterator.BatchIteratorSimple(batch_size=batch_size,\n",
    "                                             data_folder=os.path.join('..', 'data', 'validation'),\n",
    "                                             image_shape=image_shape)\n",
    "\n",
    "logger_cb = plotting_tools.LoggerPlotter()\n",
    "callbacks = [logger_cb]\n",
    "\n",
    "model.fit_generator(train_iter,\n",
    "                    steps_per_epoch = steps_per_epoch, # the number of batches per epoch,\n",
    "                    epochs = num_epochs, # the number of epochs to train for,\n",
    "                    validation_data = val_iter, # validation iterator\n",
    "                    validation_steps = validation_steps, # the number of batches to validate on\n",
    "                    callbacks=callbacks,\n",
    "                    workers = workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save your trained model weights\n",
    "weight_file_name = 'model_weights'\n",
    "model_tools.save_network(model, weight_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction<a id='prediction'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If you need to load a model which you previously trained you can uncomment the codeline that calls the function below.\n",
    "\n",
    "# weight_file_name = 'model_weights'\n",
    "# restored_model = model_tools.load_network(weight_file_name)\n",
    "# If you need to load a model which you previously trained you can uncomment the codeline that calls the following function.\n",
    "def load_weights(your_model, your_weight_filename):\n",
    "    model_path = os.path.join('..', 'data', 'weights', your_weight_filename)\n",
    "    if os.path.exists(model_path):\n",
    "        model = your_model.load_weights(model_path)\n",
    "        return model\n",
    "    else:\n",
    "        raise ValueError('No weight file found at {}'.format(model_path))\n",
    "\n",
    "# model = load_weights(model, weight_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_dir_if_not_exist(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTE only modify these lines if you have changed where data is being stored(not recommended)\n",
    "validation_path = os.path.join('..', 'data', 'validation')\n",
    "file_names = sorted(glob.glob(os.path.join(validation_path, 'images', '*.jpeg')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'prediction1'# TODO add the name of folder to save these predictions to\n",
    "output_path = os.path.join('..', 'data', 'runs', experiment_name)\n",
    "make_dir_if_not_exist(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate predictions, save in the runs, directory.\n",
    "#run_number = 'run1'\n",
    "#validation_path, output_path = model_tools.write_predictions_grade_set(model,run_number,'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name in file_names:\n",
    "    image = misc.imread(name)\n",
    "    if image.shape[0] != image_shape[0]:\n",
    "         image = misc.imresize(image,image_shape)\n",
    "    image = data_iterator.preprocess_input(image.astype(np.float32))\n",
    "    pred = model.predict_on_batch(np.expand_dims(image, 0))\n",
    "    base_name = os.path.basename(name).split('.')[0]\n",
    "    base_name = base_name + '_prediction.png'\n",
    "    misc.imsave(os.path.join(output_path, base_name), np.squeeze((pred * 255).astype(np.uint8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1055 is out of bounds for axis 1 with size 760",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-552cbe86892f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# take a look at predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvalidation_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'validation'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mim_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplotting_tools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_im_file_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_number\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mim_tuple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplotting_tools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/RoboND-Segmentation-Lab/code/utils/plotting_tools.py\u001b[0m in \u001b[0;36mget_im_file_sample\u001b[0;34m(pred_run_name, subset_name, grading_data_dir_name, n_file_names)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mshuffed_inds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mims_subset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshuffed_inds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_file_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0mmasks_subset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshuffed_inds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_file_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mims_subset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1055 is out of bounds for axis 1 with size 760"
     ]
    }
   ],
   "source": [
    "# take a look at predictions\n",
    "validation_path = 'validation'\n",
    "im_files = plotting_tools.get_im_file_sample(run_number,validation_path) \n",
    "for i in range(3):\n",
    "    im_tuple = plotting_tools.load_images(im_files[i])\n",
    "    plotting_tools.show_images(im_tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation<a id='evaluation'></a>\n",
    "Let's evaluate your model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-3307a410b6a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscoring_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/RoboND-Segmentation-Lab/code/utils/scoring_utils.py\u001b[0m in \u001b[0;36mscore_run\u001b[0;34m(gt_dir, pred_dir)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mgt_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmisc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mpred_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmisc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m127\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgt_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mpred_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "scoring_utils.score_run(validation_path, output_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
